# UniModel - ç»Ÿä¸€æ¨¡å‹æœåŠ¡å¼•æ“

[English README](README.md) | ä¸­æ–‡ç‰ˆæœ¬

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)

## ğŸš€ é¡¹ç›®æ¦‚è§ˆ

UniModel æ˜¯ä¸€ä¸ª**ç»Ÿä¸€æ¨¡å‹æœåŠ¡å¼•æ“**ï¼Œæ—¨åœ¨ä»¥æå…¶ç®€å•ã€é«˜æ•ˆã€ç»Ÿä¸€çš„æ–¹å¼ä¸ºå„ç±»AIæ¨¡å‹æä¾›æœåŠ¡èƒ½åŠ›ã€‚ä¸ä¸“é—¨åŒ–çš„è§£å†³æ–¹æ¡ˆå¦‚ vLLMï¼ˆä¸“æ³¨LLMï¼‰æˆ–å¤æ‚å¹³å°å¦‚ NVIDIA Triton ä¸åŒï¼ŒUniModel é€šè¿‡æä¾›æ—¢å¼ºå¤§åˆæ˜“ç”¨çš„ä¼ä¸šçº§æ¨¡å‹æœåŠ¡æ¥å¡«è¡¥å¸‚åœºç©ºç™½ã€‚

### æ ¸å¿ƒä»·å€¼ä¸»å¼ 

- **ğŸ”„ ç»Ÿä¸€æ€§**: é€šè¿‡å•ä¸€ã€ç»Ÿä¸€çš„APIä¸ºLLMã€è®¡ç®—æœºè§†è§‰å’Œä¼ ç»ŸMLæ¨¡å‹æä¾›æœåŠ¡
- **âš¡ é«˜æ€§èƒ½**: Rusté©±åŠ¨çš„æ ¸å¿ƒå¼•æ“ï¼Œæ™ºèƒ½æ‰¹å¤„ç†å’ŒGPUèµ„æºæ± åŒ–
- **ğŸ¯ æç®€æ€§**: ä»å¤æ‚å¹³å°ä¸­æå–æœ€å¸¸ç”¨çš„20%åŠŸèƒ½ï¼Œæ

ä¾›10å€çš„æ˜“ç”¨æ€§æå‡

* **ğŸŒ äº‘åŸç”Ÿ**: ä¸ºåˆ†å¸ƒå¼éƒ¨ç½²è€Œè®¾è®¡ï¼Œæ”¯æŒetcdæœåŠ¡å‘ç°å’ŒNATSæ¶ˆæ¯ä¼ é€’
* **ğŸ”Œ æ’ä»¶æ¶æ„**: é€šè¿‡åŠ¨æ€æ’ä»¶åŠ è½½å®ç°å¯æ‰©å±•çš„åç«¯æ”¯æŒ

## ğŸ’¡ é—®é¢˜é™ˆè¿°ä¸è§£å†³æ–¹æ¡ˆ

### å½“å‰ç—›ç‚¹

| æŒ‘æˆ˜        | ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆ       | UniModelæ–¹æ³•    |
| --------- | ------------ | ------------- |
| **å¼‚æ„æ¨¡å‹**  | æ¯ç§æ¨¡å‹ç±»å‹éœ€è¦å•ç‹¬éƒ¨ç½² | æ‰€æœ‰æ¨¡å‹ç±»å‹çš„ç»Ÿä¸€API  |
| **å¤æ‚è®¾ç½®**  | éœ€è¦ä¸“å®¶çº§åˆ«çš„é…ç½®    | ä¸€é”®éƒ¨ç½²          |
| **èµ„æºæµªè´¹**  | é™æ€èµ„æºåˆ†é…       | åŠ¨æ€åŠ è½½/å¸è½½ä¸GPUæ± åŒ– |
| **ä¾›åº”å•†é”å®š** | ç»‘å®šç‰¹å®šæ¨ç†æ¡†æ¶     | åŸºäºæ’ä»¶çš„å¤šåç«¯æ”¯æŒ    |
| **æ‰©å±•å¤æ‚æ€§** | æ‰‹åŠ¨é›†ç¾¤ç®¡ç†       | æ™ºèƒ½è´Ÿè½½å‡è¡¡çš„è‡ªåŠ¨æ‰©å±•   |

### æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆ

UniModel å®ç°äº†\*\*"é€šè¿‡ç»Ÿä¸€å®ç°ç®€åŒ–"\*\*çš„ç†å¿µï¼š

```mermaid
flowchart TB
    API[ğŸŒ Unified API Layer]

    subgraph Models
        LLM[ğŸ¤– LLM Models<br/>ï¼ˆGGUF, TRT-LLMï¼‰]
        CV[ğŸ‘ï¸ CV Models<br/>ï¼ˆONNX, PyTorchï¼‰]
        Audio[ğŸµ Audio Models<br/>ï¼ˆWhisper, etc.ï¼‰]
        ML[ğŸ“Š ML Models<br/>ï¼ˆSklearn, etc.ï¼‰]
    end

    API --> LLM
    API --> CV
    API --> Audio
    API --> ML
```

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ ç»Ÿä¸€APIæ¥å£

```bash
# ä»»ä½•æ¨¡å‹ç±»å‹éƒ½ä½¿ç”¨ç›¸åŒçš„API
curl -X POST http://localhost:8000/v1/models/llama-2-7b:predict \
  -H "Content-Type: application/json" \
  -d '{"input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}'

curl -X POST http://localhost:8000/v1/models/resnet-50:predict \
  -H "Content-Type: application/json" \
  -d '{"input": "base64_encoded_image_data"}'
```

### ğŸ”Œ å¤šåç«¯æ’ä»¶æ”¯æŒ

* **LLMåç«¯**: GGUFï¼ˆLlamaç³»åˆ—ï¼‰ã€TensorRT-LLMã€Transformers
* **é€šç”¨AI**: ONNXã€TensorFlow SavedModelã€PyTorch TorchScript
* **è‡ªå®šä¹‰åç«¯**: é€šè¿‡Python APIè½»æ¾å¼€å‘æ’ä»¶

### ğŸŒŠ æ™ºèƒ½åŠ¨æ€æ‰¹å¤„ç†

```rust
// è‡ªåŠ¨è¯·æ±‚æ‰¹å¤„ç†ä»¥è·å¾—æœ€ä½³ååé‡
let batch_config = BatchConfig {
    max_batch_size: 32,
    max_wait_time: Duration::from_millis(50),
    dynamic_padding: true,
};
```

### ğŸ“Š å†…ç½®å¯è§‚æµ‹æ€§

* PrometheusæŒ‡æ ‡å¯¼å‡º
* OpenTelemetryè¯·æ±‚è·Ÿè¸ª
* å®æ—¶æ€§èƒ½ä»ªè¡¨æ¿
* GPUåˆ©ç”¨ç‡ç›‘æ§

### ğŸ—ï¸ äº‘åŸç”Ÿæ¶æ„

* **æœåŠ¡å‘ç°**: åŸºäºetcdçš„åŠ¨æ€æ³¨å†Œ
* **æ¶ˆæ¯æ€»çº¿**: NATSç”¨äºæ§åˆ¶å¹³é¢é€šä¿¡
* **æ°´å¹³æ‰©å±•**: æ— çŠ¶æ€è®¾è®¡ä¸è´Ÿè½½å‡è¡¡
* **å¥åº·æ£€æŸ¥**: è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œæ¢å¤

## ğŸ›ï¸ æ¶æ„æ¦‚è§ˆ

```mermaid
graph LR
    %% API Gateway Layer
    subgraph API[APIç½‘å…³å±‚ï¼ˆAPI Gatewayï¼‰]
        REST[REST APIæœåŠ¡å™¨]
        GRPC[gRPC APIæœåŠ¡å™¨]
        AUTH[è®¤è¯æˆæƒï¼ˆAuthï¼‰]
    end

    %% Core Engine Layer
    subgraph CORE[æ ¸å¿ƒå¼•æ“å±‚ï¼ˆCore Engineï¼‰]
        ROUTER[è¯·æ±‚è·¯ç”±å™¨ï¼ˆRouterï¼‰]
        BATCH[æ‰¹å¤„ç†å¼•æ“ï¼ˆBatch Engineï¼‰]
        SCHED[è°ƒåº¦å™¨ï¼ˆSchedulerï¼‰]
        POOL[èµ„æºæ± ï¼ˆResource Poolï¼‰]
    end

    %% Plugin Layer
    subgraph PLUGIN[æ’ä»¶å±‚ï¼ˆPlugin Layerï¼‰]
        LLM[LLMæ’ä»¶ï¼ˆLLM Pluginï¼‰]
        CV[CVæ’ä»¶ï¼ˆCV Pluginï¼‰]
        AUDIO[éŸ³é¢‘æ’ä»¶ï¼ˆAudio Pluginï¼‰]
        CUSTOM[è‡ªå®šä¹‰æ’ä»¶ï¼ˆCustom Pluginï¼‰]
    end

    %% Infrastructure Layer
    subgraph INFRA[åŸºç¡€è®¾æ–½å±‚ï¼ˆInfrastructureï¼‰]
        ETCD[etcdæœåŠ¡å‘ç°]
        NATS[NATSæ¶ˆæ¯æ€»çº¿]
        PROM[Prometheusç›‘æ§]
        GPU[GPUèµ„æºæ± ]
    end

    %% Connections
    REST --> ROUTER
    GRPC --> ROUTER
    AUTH --> ROUTER
    ROUTER --> BATCH
    BATCH --> SCHED
    SCHED --> POOL
    POOL --> LLM
    POOL --> CV
    POOL --> AUDIO
    POOL --> CUSTOM
    SCHED --> ETCD
    SCHED --> NATS
    SCHED --> PROM
    POOL --> GPU
```

è¯¦ç»†æ¶æ„æ–‡æ¡£è¯·å‚è§ [docs/architecture.md](docs/architecture.md)ã€‚

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

* Rust 1.70+
* Python 3.8+
* Docker (å¯é€‰)
* NVIDIA GPU with CUDA 11.8+ (ç”¨äºGPUåŠ é€Ÿ)

### å®‰è£…

#### é€‰é¡¹1ï¼šä»æºç æ„å»º

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/turtacn/unimodel.git
cd unimodel

# æ„å»ºé¡¹ç›®
cargo build --release

# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# è¿è¡ŒæœåŠ¡å™¨
./target/release/unimodel-server --config config/default.yaml
```

#### é€‰é¡¹2ï¼šDocker

```bash
# æ‹‰å–å¹¶è¿è¡Œå®¹å™¨
docker run -p 8000:8000 -p 9000:9000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/config:/app/config \
  turtacn/unimodel:latest
```

### åŸºæœ¬ä½¿ç”¨

#### 1. æ³¨å†Œæ¨¡å‹

```bash
# æ³¨å†Œä¸€ä¸ªLlama-2æ¨¡å‹
curl -X POST http://localhost:8000/v1/models \
  -H "Content-Type: application/json" \
  -d '{
    "name": "llama-2-7b",
    "backend": "gguf",
    "model_path": "/models/llama-2-7b.gguf",
    "config": {
      "max_context_length": 4096,
      "gpu_layers": 35
    }
  }'
```

#### 2. è¿›è¡Œé¢„æµ‹

```bash
# æ–‡æœ¬ç”Ÿæˆ
curl -X POST http://localhost:8000/v1/models/llama-2-7b:predict \
  -H "Content-Type: application/json" \
  -d '{
    "input": "ç”¨ç®€å•çš„æœ¯è¯­è§£é‡Šé‡å­è®¡ç®—ï¼š",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

#### 3. ç›‘æ§æ€§èƒ½

```bash
# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
curl http://localhost:8000/v1/models/llama-2-7b/status

# æŸ¥çœ‹æŒ‡æ ‡
curl http://localhost:9000/metrics
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

| æŒ‡æ ‡                  | UniModel | vLLM   | Triton |
| ------------------- | -------- | ------ | ------ |
| **è®¾ç½®æ—¶é—´**            | 2åˆ†é’Ÿ      | 10åˆ†é’Ÿ   | 30åˆ†é’Ÿ   |
| **APIå¤æ‚åº¦**          | 3ä¸ªç«¯ç‚¹     | 15ä¸ªç«¯ç‚¹  | 50+ä¸ªç«¯ç‚¹ |
| **å†…å­˜æ•ˆç‡**            | 85%      | 80%    | 75%    |
| **ååé‡(tokens/sec)** | 1,250    | 1,200  | 1,300  |
| **å¤šæ¨¡å‹æ”¯æŒ**           | âœ… åŸç”Ÿ     | âŒ ä»…LLM | âœ… å¤æ‚   |

## ğŸ”§ é…ç½®

### åŸºç¡€é…ç½® (`config/default.yaml`)

```yaml
server:
  host: "0.0.0.0"
  port: 8000
  grpc_port: 9000

engine:
  max_models: 10
  default_batch_size: 8
  max_batch_wait_ms: 50

gpu:
  device_ids: [0, 1]
  memory_fraction: 0.8
  enable_pooling: true

monitoring:
  prometheus_port: 9090
  log_level: "info"
```

### é«˜çº§é…ç½®

æœ‰å…³ç”Ÿäº§éƒ¨ç½²ã€åˆ†å¸ƒå¼é…ç½®å’Œé«˜çº§åŠŸèƒ½ï¼Œè¯·å‚è§ [docs/configuration.md](docs/configuration.md)ã€‚

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
cargo test

# è¿è¡Œé›†æˆæµ‹è¯•
cargo test --test integration

# è¿è¡ŒPythonæ’ä»¶æµ‹è¯•
python -m pytest tests/

# æ€§èƒ½åŸºå‡†æµ‹è¯•
cargo bench
```

## ğŸ¤ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
cargo install cargo-watch
pip install -r requirements-dev.txt

# åœ¨å¼€å‘æ¨¡å¼ä¸‹è¿è¡Œ
cargo watch -x "run -- --config config/dev.yaml"
```

### æ’ä»¶å¼€å‘

```python
# ç¤ºä¾‹ï¼šè‡ªå®šä¹‰æ¨¡å‹æ’ä»¶
from unimodel.plugins import BasePlugin, ModelConfig

class CustomModelPlugin(BasePlugin):
    def load_model(self, config: ModelConfig) -> None:
        # æ‚¨çš„æ¨¡å‹åŠ è½½é€»è¾‘
        pass
    
    def predict(self, input_data: dict) -> dict:
        # æ‚¨çš„é¢„æµ‹é€»è¾‘
        return {"output": "prediction_result"}
```

## ğŸ“š æ–‡æ¡£

* [æ¶æ„è®¾è®¡](docs/architecture.md)
* [APIå‚è€ƒ](docs/api.md)
* [æ’ä»¶å¼€å‘æŒ‡å—](docs/plugins.md)
* [éƒ¨ç½²æŒ‡å—](docs/deployment.md)
* [æ€§èƒ½è°ƒä¼˜](docs/performance.md)

## ğŸ—ºï¸ è·¯çº¿å›¾

* [ ] **v0.1.0**: å…·æœ‰åŸºæœ¬æ’ä»¶æ”¯æŒçš„æ ¸å¿ƒå¼•æ“
* [ ] **v0.2.0**: åˆ†å¸ƒå¼éƒ¨ç½²åŠŸèƒ½
* [ ] **v0.3.0**: é«˜çº§æ‰¹å¤„ç†å’Œç¼“å­˜
* [ ] **v0.4.0**: è‡ªåŠ¨æ‰©å±•å’Œè´Ÿè½½å‡è¡¡
* [ ] **v1.0.0**: å…·æœ‰å®Œæ•´åŠŸèƒ½é›†çš„ç”Ÿäº§å°±ç»ªç‰ˆæœ¬

## ğŸ“„ è®¸å¯è¯

æ­¤é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

* [vLLM](https://github.com/vllm-project/vllm) æä¾›çš„LLMæœåŠ¡çµæ„Ÿ
* [NVIDIA Triton](https://github.com/triton-inference-server/server) çš„å¤šåç«¯æ¶æ„æ¨¡å¼
* [etcd](https://etcd.io/) å’Œ [NATS](https://nats.io/) æä¾›çš„äº‘åŸç”ŸåŸºç¡€è®¾æ–½ç»„ä»¶

---

**ç”±UniModelå›¢é˜Ÿç”¨â¤ï¸æ„å»º**

*å¦‚æœ‰é—®é¢˜ã€å»ºè®®æˆ–éœ€è¦æ”¯æŒï¼Œè¯·æäº¤issueæˆ–åŠ å…¥æˆ‘ä»¬çš„ [Discordç¤¾åŒº](https://discord.gg/unimodel)ã€‚*

````

## ä»£ç èƒ½åŠ›å±•ç¤ºç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç»Ÿä¸€APIè°ƒç”¨å±•ç¤º
```bash
#!/bin/bash
# å±•ç¤ºUniModelç»Ÿä¸€APIçš„å¼ºå¤§èƒ½åŠ›

echo "=== UniModelç»Ÿä¸€APIæ¼”ç¤º ==="

# 1. æ³¨å†ŒLLMæ¨¡å‹
echo "1. æ³¨å†ŒLlama-2æ¨¡å‹..."
curl -X POST http://localhost:8000/v1/models \
  -H "Content-Type: application/json" \
  -d '{
    "name": "llama-2-7b",
    "backend": "gguf",
    "model_path": "/models/llama-2-7b.gguf"
  }' | jq .

# 2. æ³¨å†ŒCVæ¨¡å‹
echo "2. æ³¨å†ŒResNet-50æ¨¡å‹..."
curl -X POST http://localhost:8000/v1/models \
  -H "Content-Type: application/json" \
  -d '{
    "name": "resnet-50",
    "backend": "onnx",
    "model_path": "/models/resnet50.onnx"
  }' | jq .

# 3. åŒæ ·çš„APIè°ƒç”¨ä¸åŒç±»å‹æ¨¡å‹
echo "3. è°ƒç”¨LLMæ¨¡å‹..."
curl -X POST http://localhost:8000/v1/models/llama-2-7b:predict \
  -H "Content-Type: application/json" \
  -d '{"input": "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ "}' | jq .

echo "4. è°ƒç”¨CVæ¨¡å‹..."
curl -X POST http://localhost:8000/v1/models/resnet-50:predict \
  -H "Content-Type: application/json" \
  -d '{"input": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQ..."}' | jq .

echo "=== æ¼”ç¤ºå®Œæˆ ==="
````

### ç¤ºä¾‹2ï¼šRustæ ¸å¿ƒæ€§èƒ½å±•ç¤º

```rust
// å±•ç¤ºUniModelæ ¸å¿ƒæ‰¹å¤„ç†å¼•æ“çš„æ€§èƒ½ä¼˜åŠ¿
use std::time::Instant;
use tokio::time::Duration;

#[tokio::main]
async fn main() {
    let start = Instant::now();
    
    // æ¨¡æ‹Ÿ1000ä¸ªå¹¶å‘è¯·æ±‚
    let tasks: Vec<_> = (0..1000)
        .map(|i| {
            tokio::spawn(async move {
                let client = reqwest::Client::new();
                let response = client
                    .post("http://localhost:8000/v1/models/llama-2-7b:predict")
                    .json(&serde_json::json!({
                        "input": format!("è¯·æ±‚ {} çš„å¤„ç†", i),
                        "max_tokens": 50
                    }))
                    .send()
                    .await;
                
                match response {
                    Ok(resp) => println!("è¯·æ±‚ {} æˆåŠŸ: {}", i, resp.status()),
                    Err(e) => println!("è¯·æ±‚ {} å¤±è´¥: {}", i, e),
                }
            })
        })
        .collect();
    
    // ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
    for task in tasks {
        task.await.unwrap();
    }
    
    let duration = start.elapsed();
    println!("1000ä¸ªè¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {:?}", duration);
    println!("å¹³å‡QPS: {:.2}", 1000.0 / duration.as_secs_f64());
}
```

### ç¤ºä¾‹3ï¼šPythonæ’ä»¶å¼€å‘å±•ç¤º

```python
# å±•ç¤ºå¦‚ä½•è½»æ¾å¼€å‘è‡ªå®šä¹‰æ¨¡å‹æ’ä»¶
from unimodel.plugins import BasePlugin, ModelConfig, PredictRequest, PredictResponse
import torch
import transformers

class CustomBertPlugin(BasePlugin):
    """è‡ªå®šä¹‰BERTæ¨¡å‹æ’ä»¶ç¤ºä¾‹"""
    
    def __init__(self):
        super().__init__()
        self.model = None
        self.tokenizer = None
    
    def load_model(self, config: ModelConfig) -> None:
        """åŠ è½½BERTæ¨¡å‹"""
        print(f"Loading BERT model from {config.model_path}")
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_path)
        self.model = transformers.AutoModel.from_pretrained(config.model_path)
        self.model.eval()
    
    def predict(self, request: PredictRequest) -> PredictResponse:
        """æ‰§è¡Œé¢„æµ‹"""
        # é¢„å¤„ç†
        inputs = self.tokenizer(
            request.input,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
        
        return PredictResponse(
            output=embeddings,
            metadata={
                "model_name": "custom-bert",
                "input_length": len(request.input),
                "embedding_dim": len(embeddings)
            }
        )
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "name": "Custom BERT Plugin",
            "version": "1.0.0",
            "supported_formats": ["text"],
            "description": "Custom BERT model for text embedding"
        }

# æ³¨å†Œæ’ä»¶
plugin = CustomBertPlugin()
```

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†UniModelçš„æ ¸å¿ƒèƒ½åŠ›ï¼š

1. **ç»Ÿä¸€API**: æ— è®ºåç«¯æ¨¡å‹ç±»å‹å¦‚ä½•ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„æ¥å£
2. **é«˜æ€§èƒ½**: Rustæ ¸å¿ƒå¼•æ“æ”¯æŒé«˜å¹¶å‘å’Œä½å»¶è¿Ÿ
3. **æ˜“æ‰©å±•**: Pythonæ’ä»¶å¼€å‘ç®€å•ç›´è§‚ï¼Œå‡ åè¡Œä»£ç å³å¯æ”¯æŒæ–°æ¨¡å‹ç±»å‹